---
title: "Activity 6 - Logistic Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r libraries, message=FALSE}
library(tidyverse)
library(tidymodels)
library(GGally)
```

## The Data
```{r Data, message=FALSE}
resume <- readr::read_csv("https://www.openintro.org/data/csv/resume.csv")
```
The variable of interest is `received_callback`, which is a binary variable.

```{r target visualization}
resume %>% 
  ggplot(aes(x=received_callback==1)) + 
  geom_bar() + 
  theme_bw() + 
  ylab("Count") + 
  xlab("Received Call Back") + 
  ggtitle("# of Received call Back")
```

```{r summary table}
# TODO Add Percentages
resume %>% 
  group_by(received_callback) %>% 
  summarise(n()) %>% 
  knitr::kable()
```
The probability of someone getting a call back from their resume is 8.05% or 392/(4478+392). The odds of someone getting a call back is 8.75% or 392/4478.

## Logistic Regression
```{r summary black/white}
resume %>% 
  group_by(received_callback, race) %>% 
  summarise(n()) %>% 
  knitr::kable()
```
The probability that a randomly selected résumé/person perceived as Black will be called back is 6.45%.
The odds that a randomly selected résumé/person perceived as Black will be called back is 6.89%.

### Logistic Model
```{r logistic model}
# The {tidymodels} method for logistic regression requires that the response be a factor variable
resume <- resume %>% 
  mutate(received_callback = as.factor(received_callback))

resume_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(received_callback ~ race, data = resume, family = "binomial")

tidy(resume_mod) %>% 
  knitr::kable(digits = 3)
```
The regression equation corresponding to résumés/persons perceived as White is `received_callback = 0.438(racewhite) - 2.675`. 

The *simplified* estimated regression equation corresponding to résumés/persons perceived as Black is `received_callback = ???? (raceblack) + 2.675``

## Multinomial Logistic Regression

Select records in Chicago and select the relevant variables. Rename `gender` to `sex` and converts `race` to capital letters and `sex` to full words.
```{r select variables and data in chicago}
resume_select <- resume %>% 
  rename(sex = gender) %>% 
  filter(job_city == "Chicago") %>% 
  mutate(race = case_when(
         race == "white" ~ "White",
         TRUE ~ "Black"
       ),
       sex = case_when(
         sex == "f" ~ "female",
         TRUE ~ "male"
       )) %>% 
  select(received_callback, years_experience, race, sex)
```

## Relationship Exploration

There is a visible difference between callbacks for `race` and `sex`.
```{r relationship exploration}
resume_select %>% 
  ggbivariate(outcome = "received_callback") + 
  theme_bw()
```

## Fitting the Model
```{r fit model}
mult_log_mod <- glm(received_callback ~ years_experience + race + sex, data = resume_select, family = "binomial")

tidy(mult_log_mod) %>% 
  knitr::kable(digits = 3)
```

```{r odds}
tidy(mult_log_mod, exponentiate = TRUE) %>% 
  knitr::kable(digits = 3)
```
For each additional year of experience for an applicant in Chicago, we expect the odds of an applicant receiving a call back to increase by 1.046%. Assuming applicants have similar time in spent in college, similar inferred races, and similar inferred sex.


## Assessing Model Fit
```{r assess model fit}
# To store residuals and create row number variable
mult_log_aug <- augment(mult_log_mod, type.predict = "response", 
                      type.residuals = "deviance") %>% 
                      mutate(id = row_number())

# Plot residuals vs fitted values
ggplot(data = mult_log_aug, aes(x = .fitted, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "Fitted values", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. fitted")

# Plot residuals vs row number
ggplot(data = mult_log_aug, aes(x = id, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "id", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. id")
```

